{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behavioral analyses FEED project\n",
    "Replication of Max' analyses.\n",
    "\n",
    "To do:\n",
    "* Proper between-subject cross-validation\n",
    "* Between-subject noise ceiling\n",
    "* Distance-based features (static and dynamic, i.e., relative to frame 0)\n",
    "* Other evaluation metric (brier?)\n",
    "* Rolling count\n",
    "* kleur modellen (Martinez paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os.path as op\n",
    "from glob import glob\n",
    "\n",
    "# sklearn stuff\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import roc_auc_score, make_scorer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# models\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Other\n",
    "from tqdm import tqdm_notebook\n",
    "from skimage.transform import rescale\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load(sub, target, lags=5):\n",
    "    \n",
    "    f = f'../behav_data/preproc/sub-{sub}_task-expressive_ratings.tsv'\n",
    "    data = pd.read_csv(f, sep='\\t', index_col=0)\n",
    "    \n",
    "    for rep in [1, 2, 3]:\n",
    "        for ses in [1, 2, 3]:\n",
    "            for block in np.arange(1, 15):\n",
    "                tmp = data.query(\"rep == @rep & session == @ses & block == @block\")\n",
    "                if tmp.shape[0] == 0:\n",
    "                    continue\n",
    "                \n",
    "                tmp['rating_t_min_1'] = np.r_[np.nan, tmp['rating'].iloc[:-1]]\n",
    "                data.loc[tmp.index, 'rating_t_min_1'] = tmp['rating_t_min_1']\n",
    "                for lag in np.arange(2, lags+1):\n",
    "                    tmp[f'rating_t_min_{lag}'] = np.r_[np.nan, tmp[f'rating_t_min_{lag-1}'].iloc[:-1]]\n",
    "                    data.loc[tmp.index, f'rating_t_min_{lag}'] = tmp[f'rating_t_min_{lag}']\n",
    "                    \n",
    "    data = data.set_index(data.trial_type.astype(str) + '_rep-' + data.rep.astype(str))\n",
    "    unmelted = pd.pivot(data, columns='rating_type', values='rating')\n",
    "    data = data.query(\"rating_type == @target\")\n",
    "    data = pd.concat((data, unmelted), axis=1)\n",
    "    data = data.sort_values(['rep', 'session', 'run', 'block', 'trial'], axis=0)        \n",
    "    return data\n",
    "\n",
    "\n",
    "def get_Xy_from_tsv(sub, feature_space='AU', target='emotion', remove_gva=True, theory_kernel=None,\n",
    "                    n_comp=50, lags=5):\n",
    "    \n",
    "    data = _load(sub, target, lags)\n",
    "    data = data.query(\"data_split == 'train'\")\n",
    "    data, test = train_test_split(data, test_size=0.15, random_state=24, stratify=data[target])\n",
    "    \n",
    "    # TO FIX: only second argmax from double trials\n",
    "\n",
    "    if remove_gva:\n",
    "        data = data.query(\"emotion != 'Geen van allen'\")    \n",
    "    \n",
    "    to_return = dict()\n",
    "    y = data.loc[:, target]\n",
    "    d2e = dict(Bang='fear', Blij='happy', Boos='angry', Verdrietig='sad', Verrassing='surprise', Walging='disgust')\n",
    "    to_return['y'] = y.map(d2e)\n",
    "    to_return['intensity'] = data.loc[:, 'rating_intensity']\n",
    "    \n",
    "    if feature_space == 'autocorr':\n",
    "        cols2use = [f'rating_t_min_{i}' for i in np.arange(1, lags + 1)]\n",
    "        data = data.loc[:, cols2use]\n",
    "        for col in cols2use:\n",
    "            data[col] = data[col].map(d2e)\n",
    "        \n",
    "        # only if target == 'emotion'\n",
    "        ohe = OneHotEncoder(sparse=False)\n",
    "        cats = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise']\n",
    "        ohe.fit(np.array(cats)[:, np.newaxis])\n",
    "\n",
    "        stack = []\n",
    "        for i, col in enumerate(data.columns):\n",
    "            tmp = np.zeros((data.shape[0], 6))\n",
    "            not_nan = ~data[col].isna()\n",
    "            tmp[not_nan, :] = ohe.transform(data[col].values[not_nan, np.newaxis])\n",
    "            colnames = [cat + f'_t_min_{i+1}' for cat in cats]\n",
    "            tmp = pd.DataFrame(tmp, columns=colnames, index=data.index)\n",
    "            stack.append(tmp)\n",
    "            \n",
    "        to_return['X'] = pd.concat(stack, axis=1)\n",
    "        return to_return\n",
    "    \n",
    "    if feature_space in ['pca', 'nmf', 'pcadiff', 'nmfdiff']:\n",
    "        if feature_space in ['pca', 'nmf']:\n",
    "            X_red = pd.read_csv(f'../behav_data/features/model-{feature_space}_scale-10_frame-14_features.tsv', sep='\\t', index_col=0)\n",
    "            X_red = X_red.loc[data['trial_type'], :].iloc[:, :n_comp]\n",
    "            to_return['X'] = X_red.set_index(data.index)         \n",
    "        else:\n",
    "            X_red0 = pd.read_csv(f'../behav_data/features/model-{feature_space[:3]}_scale-10_frame-0_features.tsv', sep='\\t', index_col=0)\n",
    "            X_red14 = pd.read_csv(f'../behav_data/features/model-{feature_space[:3]}_scale-10_frame-14_features.tsv', sep='\\t', index_col=0)\n",
    "            X_red0 = X_red0.loc[data['trial_type'], :].iloc[:, :n_comp]\n",
    "            X_red14 = X_red14.loc[data['trial_type'], :].iloc[:, :n_comp]\n",
    "            X_red = X_red14 - X_red0\n",
    "            to_return['X'] = X_red.set_index(data.index)\n",
    "        return to_return\n",
    "\n",
    "    if feature_space == 'circumplex':\n",
    "        to_return['X'] = data.loc[:, ['arousal', 'valence']]\n",
    "        return to_return\n",
    "    \n",
    "    if 'AU' in feature_space or theory_kernel is not None:\n",
    "        X_AU = data.loc[:, [col for col in data.columns if 'AU' in col]]\n",
    "        X_AU = X_AU.drop('N_AUs', axis=1)\n",
    "\n",
    "        if feature_space == 'AU' or theory_kernel is not None:\n",
    "            to_return['X'] = X_AU\n",
    "            return to_return\n",
    "        \n",
    "        if 'AUxAU' in feature_space:\n",
    "            poly = PolynomialFeatures(degree=2, interaction_only=True , include_bias=False)\n",
    "            X_AUxAU = poly.fit_transform(X_AU.values)\n",
    "            X_AUxAU = pd.DataFrame(X_AUxAU, columns=poly.get_feature_names(X_AU.columns), index=data.index)\n",
    "        \n",
    "        if feature_space == 'AUxAU':\n",
    "            to_return['X'] = X_AUxAU\n",
    "            return to_return\n",
    "        \n",
    "    if 'SA' in feature_space:\n",
    "        f = f'../behav_data/preproc/sub-{sub}_task-neutral_ratings.tsv'\n",
    "        data_sa = pd.read_csv(f, sep='\\t')\n",
    "        data_sa = data_sa.set_index(data_sa.trial_type.astype(str) + '_rep-' + data_sa.rep.astype(str))\n",
    "        unmelted = pd.pivot(data_sa, columns='rating_type', values='rating')\n",
    "        \n",
    "        data_sa = pd.concat((data_sa, unmelted), axis=1)\n",
    "        cols = ['trustworthiness', 'dominance', 'attractiveness', 'valence', 'arousal']\n",
    "        data_sa = data_sa.loc[:, cols]\n",
    "        X_SA = data_sa.loc[data.face_id, :]\n",
    "        X_SA = X_SA.set_index(data.index)\n",
    "        \n",
    "        if feature_space == 'SA':\n",
    "            to_return['X'] = X_SA\n",
    "            return to_return\n",
    "        \n",
    "        elif feature_space == 'AU+SA':\n",
    "            to_return['X'] = pd.concat((X_AU, X_SA), axis=1)\n",
    "            return to_return\n",
    "        \n",
    "        elif feature_space == 'AUxAU+SA':\n",
    "            to_return['X'] = pd.concat((X_AUxAU, X_SA), axis=1)\n",
    "            return to_return\n",
    "\n",
    "    if feature_space == 'face_attributes':\n",
    "        X_fa = data.loc[:, ['age', 'gender_F', 'gender_M']]\n",
    "        to_return['X'] = X_fa\n",
    "        return to_return\n",
    "        \n",
    "    if feature_space == 'nuisance':\n",
    "        X_nuis = data.loc[:, ['session', 'run', 'block', 'trial', 'onset', 'rating_RT']]\n",
    "        to_return['X'] = X_nuis\n",
    "        return to_return\n",
    "    \n",
    "    # If we're here, there is a mistake!\n",
    "    raise ValueError(f\"Could not load data for unknown feature_space {feature_space}!\")\n",
    "\n",
    "\n",
    "def brier_score(y_true, y_pred, scale=True, per_class=False):\n",
    "    \n",
    "    if y_true.ndim == 1:\n",
    "        ohe = OneHotEncoder(categories='auto', sparse=False)\n",
    "        y_true = ohe.fit_transform(y_true[:, np.newaxis])\n",
    "\n",
    "    if per_class:\n",
    "        brier = ((y_pred - y_true) ** 2).mean(axis=0)\n",
    "    else:    \n",
    "        brier = np.sum((y_pred - y_true) ** 2, axis=1).mean()\n",
    "    \n",
    "    if scale:\n",
    "        if per_class:\n",
    "            brier_max = np.mean(y_pred, axis=0) * np.mean(1 - y_pred, axis=0)\n",
    "        else:\n",
    "            brier_max = np.sum(np.mean(y_pred, axis=0) * np.mean(1 - y_pred, axis=0))\n",
    "        \n",
    "        brier = 1 - (brier / brier_max)\n",
    "        \n",
    "    return brier\n",
    "\n",
    "\n",
    "def tjur_score(y_true, y_pred, per_class=False):\n",
    "\n",
    "    if y_true.ndim == 1:\n",
    "        ohe = OneHotEncoder(categories='auto', sparse=False)\n",
    "        y_true = ohe.fit_transform(y_true[:, np.newaxis])\n",
    "\n",
    "    k = y_true.shape[1]\n",
    "    score_per_class = np.zeros(k)\n",
    "    for i in range(k):\n",
    "        score_0 = y_pred[y_true[:, i] == 0, i].mean()\n",
    "        score_1 = y_pred[y_true[:, i] == 1, i].mean()\n",
    "        score_per_class[i] = score_1 - score_0\n",
    "    \n",
    "    if per_class:\n",
    "        return score_per_class\n",
    "    else:\n",
    "        return score_per_class.mean()\n",
    "    \n",
    "\n",
    "def roc_auc_score_per_class(y_true, y_pred, per_class=True):\n",
    "    \n",
    "    if y_true.ndim == 1:\n",
    "        ohe = OneHotEncoder(categories='auto', sparse=False)\n",
    "        y_true = ohe.fit_transform(y_true[:, np.newaxis])\n",
    "\n",
    "    if per_class:\n",
    "        scores = np.zeros(y_true.shape[1])\n",
    "        for i in range(y_true.shape[1]):\n",
    "            scores[i] = roc_auc_score(y_true[:, i], y_pred[:, i])\n",
    "\n",
    "        return np.array(scores)\n",
    "    else:\n",
    "        return roc_auc_score(y_true, y_pred)\n",
    "\n",
    "def cross_val_predict_and_score(estimator, X, y, cv, scoring, per_class=True, X_cv=None, y_cv=None):\n",
    "    \n",
    "    n_class = y.unique().size\n",
    "    classes = np.sort(y.unique())\n",
    "    \n",
    "    if X_cv is not None and y_cv is not None:\n",
    "        cv_gen = cv.split(X_cv, y_cv)\n",
    "        preds = pd.DataFrame(np.zeros((y_cv.size, n_class)), columns=classes, index=X_cv.index)\n",
    "    else:\n",
    "        cv_gen = cv.split(X, y)\n",
    "        preds = pd.DataFrame(np.zeros((y.size, n_class)), columns=classes, index=X.index)\n",
    "    \n",
    "    estimators = []\n",
    "    for i, (train_idx_int, test_idx_int) in enumerate(cv_gen):\n",
    "        \n",
    "        if X_cv is not None and y_cv is not None:        \n",
    "            train_idx = X.loc[X_cv.index[train_idx_int], :].dropna().index\n",
    "            \n",
    "            test_idx = X.loc[X_cv.index[test_idx_int], :].dropna().index\n",
    "        else:\n",
    "            train_idx = X.index[train_idx_int]\n",
    "            test_idx = X.index[test_idx_int]\n",
    "\n",
    "        estimator.fit(X.loc[train_idx, :], y.loc[train_idx])\n",
    "        \n",
    "        if X_cv is not None and y_cv is not None:\n",
    "            test_idx = X_cv.index[test_idx_int]\n",
    "            to_pred = X_cv.loc[test_idx, :]\n",
    "        else:\n",
    "            to_pred = X.loc[test_idx, :]\n",
    "        \n",
    "        preds.loc[test_idx, :] = estimator.predict_proba(to_pred)        \n",
    "        estimators.append(estimator)\n",
    "\n",
    "    y2use = y_cv if y_cv is not None else y\n",
    "    scores = scoring(y2use.values, preds.values, per_class=per_class)\n",
    "    \n",
    "    coef = np.zeros((len(estimators), n_class, X.shape[1]))\n",
    "    for i, est in enumerate(estimators):\n",
    "        coef[i, :, :] = est.steps[-1][1].coef_\n",
    "    \n",
    "    coef = np.mean(coef, axis=0)\n",
    "    return scores, coef\n",
    "\n",
    "\n",
    "scorers = dict(\n",
    "    brier_score=dict(\n",
    "        scorer=make_scorer(brier_score, needs_proba=True),\n",
    "        func=brier_score\n",
    "    ),\n",
    "    tjur_score=dict(\n",
    "        scorer=make_scorer(tjur_score, needs_proba=True),\n",
    "        func=tjur_score\n",
    "    ),\n",
    "    auc_score=dict(\n",
    "        scorer=make_scorer(roc_auc_score_per_class, needs_proba=True),\n",
    "        func=roc_auc_score_per_class\n",
    "    )\n",
    ")\n",
    "\n",
    "clfs = dict(\n",
    "    svm=SVC(kernel='linear', probability=True, class_weight='balanced', decision_function_shape='ovr'),\n",
    "    lsvm=CalibratedClassifierCV(LinearSVC(class_weight='balanced', multi_class='ovr')),\n",
    "    lr=LogisticRegression(penalty='elasticnet', class_weight='balanced', solver='saga', l1_ratio=0.5),\n",
    "    rf=RandomForestClassifier(class_weight='balanced'),\n",
    ")\n",
    "\n",
    "gridsearch = dict(\n",
    "    svm={'C': [0.001, 0.01, 0.1, 1, 10]},\n",
    "    lr={'C': [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "        'l1_ratio': [0, 0.25, .5, 0.75, 1]},\n",
    "    rf={'n_estimators': [10, 100, 1000]}    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:22: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:109: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (500, 72), indices imply (100, 72)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0e5167f01de0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_Xy_from_tsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'01'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'SA'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-7df9dd3ad76a>\u001b[0m in \u001b[0;36mget_Xy_from_tsv\u001b[0;34m(sub, feature_space, target, remove_gva, theory_kernel, n_comp, lags)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0munmelted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_sa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rating_type'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rating'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mdata_sa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_sa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munmelted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'trustworthiness'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dominance'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'attractiveness'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'valence'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'arousal'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mdata_sa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_sa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    256\u001b[0m     )\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             new_data = concatenate_block_managers(\n\u001b[0;32m--> 473\u001b[0;31m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m             )\n\u001b[1;32m    475\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m   2057\u001b[0m         \u001b[0mblocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2059\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, blocks, axes, do_integrity_check)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_integrity_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verify_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_verify_integrity\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verify_integrity\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mmgr_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0mconstruction_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtot_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtot_items\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             raise AssertionError(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mconstruction_error\u001b[0;34m(tot_items, block_shape, axes, e)\u001b[0m\n\u001b[1;32m   1717\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Empty data passed with indices specified.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m     raise ValueError(\n\u001b[0;32m-> 1719\u001b[0;31m         \u001b[0;34m\"Shape of passed values is {0}, indices imply {1}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpassed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimplied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1720\u001b[0m     )\n\u001b[1;32m   1721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (500, 72), indices imply (100, 72)"
     ]
    }
   ],
   "source": [
    "data = get_Xy_from_tsv('01', feature_space='SA')\n",
    "data['y'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise ceiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "METRIC = 'auc_score'\n",
    "TARGET = 'emotion'\n",
    "\n",
    "files = sorted(glob('../behav_data/preproc/*task-expressive*.tsv'))\n",
    "cats = ['happy', 'angry', 'fear', 'sad', 'surprise', 'disgust']\n",
    "le = LabelEncoder()\n",
    "le.fit(cats)\n",
    "ohe = OneHotEncoder(categories='auto', sparse=False)\n",
    "ohe.fit(le.transform(le.classes_)[:, np.newaxis])\n",
    "print('\\t\\t', le.classes_)\n",
    "\n",
    "ceiling = {'sub': [], 'ceiling': [], 'emo': []}\n",
    "for i, f in enumerate(files):\n",
    "    \n",
    "    sub = op.basename(f).split('_')[0].split('-')[1]\n",
    "    data = _load(sub, TARGET)\n",
    "    data = data.query(\"emotion != 'Geen van allen'\") \n",
    "    data = data.set_index(\"trial_type\")\n",
    "    \n",
    "    d2e = dict(Bang='fear', Blij='happy', Boos='angry', Verdrietig='sad', Verrassing='surprise', Walging='disgust')\n",
    "    data['emotion'] = [d2e[s] for s in data['emotion']]\n",
    "\n",
    "    dup_idx = data.query(\"rep == 2\").index\n",
    "    dup_df = pd.DataFrame({\n",
    "        'rep1': data.query(\"rep == 1\").loc[dup_idx, 'emotion'],\n",
    "        'rep2': data.query(\"rep == 2\").loc[dup_idx, 'emotion'],\n",
    "        'rep3': data.query(\"rep == 3\").loc[dup_idx, 'emotion']\n",
    "    })\n",
    "    \n",
    "    dup_df = dup_df.dropna(how='any', axis=0)\n",
    "    y_flat = data.loc[dup_df.index, 'emotion']\n",
    "\n",
    "    y = ohe.transform(le.transform(y_flat)[:, np.newaxis]).astype(int)\n",
    "    y2d = le.transform(dup_df.values.ravel()).reshape(dup_df.shape)\n",
    "    counts = np.apply_along_axis(np.bincount, 1, y2d, minlength=6)\n",
    "    best_pred = np.zeros_like(counts, dtype=float)\n",
    "    for ii in range(counts.shape[0]):\n",
    "        opt_class = np.where(counts[ii, :] == counts[ii, :].max())[0]\n",
    "        best_pred[ii, opt_class] = 1 / len(opt_class)\n",
    "\n",
    "    best_pred_rep = []\n",
    "    for i in range(best_pred.shape[0]):\n",
    "        x = best_pred[np.newaxis, i, :]\n",
    "        best_pred_rep.append(np.r_[x, x, x])\n",
    "    best_pred_rep = np.vstack(best_pred_rep)\n",
    "    \n",
    "    ceil = scorers[METRIC]['func'](y, best_pred_rep, per_class=True)\n",
    "    print(f\"{sub} ceiling: {np.round(ceil, 6)}\")\n",
    "    \n",
    "    ceiling['sub'].extend([sub] * 6)\n",
    "    ceiling['ceiling'].extend(ceil.tolist())\n",
    "    ceiling['emo'].extend(le.classes_)\n",
    "    \n",
    "ceiling = pd.DataFrame(ceiling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different feature spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "METRIC = 'auc_score'\n",
    "CLF = 'lr'\n",
    "N_JOBS = 5\n",
    "GS = False\n",
    "SD = True\n",
    "N_SPLITS = 2\n",
    "PER_CLASS = True\n",
    "\n",
    "if GS:\n",
    "    clf = GridSearchCV(\n",
    "        estimator=clfs[CLF],\n",
    "        param_grid=gridsearch[CLF],\n",
    "        cv=5,\n",
    "        iid=True,\n",
    "        n_jobs=N_JOBS,\n",
    "        scoring=scorers[METRIC]['scorer']\n",
    "    )\n",
    "else:\n",
    "    clf = clfs[CLF]\n",
    "\n",
    "if SD:\n",
    "    pipe = make_pipeline(StandardScaler(), clf)\n",
    "else:\n",
    "    pipe = clfs[CLF]\n",
    "\n",
    "cv = StratifiedKFold(n_splits=N_SPLITS)\n",
    "\n",
    "files = sorted(glob('../behav_data/preproc/*task-expressive*.tsv'))\n",
    "results = {'sub': [], 'score': [], 'model': [], 'emo': [], 'feature_space': [], 'type': []}\n",
    "coef_dfs = dict()\n",
    "\n",
    "# also possible: 'subset_AU'\n",
    "for feature_space in ['SA', 'circumplex', 'AU', 'AUxAU', 'AU+SA', 'AUxAU+SA', 'nuisance', 'face_attributes']:\n",
    "    coef_dfs[feature_space] = []\n",
    "    print('\\n%s' % feature_space)\n",
    "    for i, f in enumerate(files):\n",
    "\n",
    "        sub = op.basename(f).split('_')[0]\n",
    "        dat = get_Xy_from_tsv(sub.split('-')[1], feature_space=feature_space)\n",
    "        X, y, intensity = dat['X'], dat['y'], dat['intensity']\n",
    "        scores, coef = cross_val_predict_and_score(pipe, X, y, cv=cv, scoring=scorers[METRIC]['func'])\n",
    "        print(f\"{sub}:\\t {np.round(scores, 3)}\")\n",
    "        \n",
    "        if hasattr(scores, '__iter__'):\n",
    "            mult = scores.size\n",
    "        else:\n",
    "            scores = [scores]\n",
    "            mult = 1\n",
    "\n",
    "        coef_df = pd.DataFrame(data=coef, columns=X.columns)\n",
    "        coef_df['sub'] = sub\n",
    "        coef_df['emo'] = le.classes_[:mult]\n",
    "        coef_dfs[feature_space].append(coef_df)\n",
    "        \n",
    "        results['sub'].extend([sub] * mult)\n",
    "        results['score'].extend(scores)\n",
    "        results['model'].extend(['self'] * mult)\n",
    "        results['emo'].extend(le.classes_[:mult])\n",
    "        results['feature_space'].extend([feature_space] * mult)\n",
    "        results['type'].extend(['AU-based'] * mult)\n",
    "        \n",
    "    coef_dfs[feature_space] = pd.concat(coef_dfs[feature_space], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "sns.catplot(x='feature_space', y='score', hue='emo', data=results_df,\n",
    "            kind=\"bar\", aspect=3, height=5, ci='sd')\n",
    "plt.axhline(0.5, ls='--', c='k')\n",
    "ax = sns.stripplot(x='feature_space', y='score', hue='emo', edgecolor='black',\n",
    "                   data=results_df, dodge=True, jitter=False)\n",
    "ax.legend_.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x='emo', y='score', hue='feature_space', data=results_df,\n",
    "            kind=\"bar\", aspect=3, height=5, ci='sd')\n",
    "plt.axhline(0.5, ls='--', c='k')\n",
    "ax = sns.stripplot(x='emo', y='score', hue='feature_space', edgecolor='black',\n",
    "                   data=results_df, dodge=True, jitter=False)\n",
    "ax.legend_.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.melt(coef_dfs['AU'], id_vars=['sub', 'emo'],var_name='feature', value_name='coef')\n",
    "sns.catplot(x='feature', y='coef', data=tmp, hue='emo',\n",
    "            kind=\"bar\", aspect=3, height=5, ci='sd')\n",
    "plt.axhline(0., ls='--', c='k')\n",
    "ax = sns.stripplot(x='feature', y='coef', hue='emo', edgecolor='black',\n",
    "                   data=tmp, dodge=True, jitter=False)\n",
    "ax.legend_.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.melt(coef_dfs['circumplex'], id_vars=['sub', 'emo'],var_name='feature', value_name='coef')\n",
    "sns.catplot(x='feature', y='coef', data=tmp, hue='emo',\n",
    "            kind=\"bar\", aspect=3, height=5, ci='sd')\n",
    "plt.axhline(0., ls='--', c='k')\n",
    "ax = sns.stripplot(x='feature', y='coef', hue='emo', edgecolor='black',\n",
    "                   data=tmp, dodge=True, jitter=False)\n",
    "ax.legend_.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.melt(coef_dfs['SA'], id_vars=['sub', 'emo'],var_name='feature', value_name='coef')\n",
    "sns.catplot(x='feature', y='coef', data=tmp, hue='emo',\n",
    "            kind=\"bar\", aspect=3, height=5, ci='sd')\n",
    "plt.axhline(0., ls='--', c='k')\n",
    "ax = sns.stripplot(x='feature', y='coef', hue='emo', edgecolor='black',\n",
    "                   data=tmp, dodge=True, jitter=False)\n",
    "ax.legend_.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.melt(coef_dfs['nuisance'], id_vars=['sub', 'emo'],var_name='feature', value_name='coef')\n",
    "sns.catplot(x='feature', y='coef', data=tmp, hue='emo',\n",
    "            kind=\"bar\", aspect=3, height=5, ci='sd')\n",
    "plt.axhline(0., ls='--', c='k')\n",
    "ax = sns.stripplot(x='feature', y='coef', hue='emo', edgecolor='black',\n",
    "                   data=tmp, dodge=True, jitter=False)\n",
    "ax.legend_.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.melt(coef_dfs['face_attributes'], id_vars=['sub', 'emo'],var_name='feature', value_name='coef')\n",
    "sns.catplot(x='feature', y='coef', data=tmp, hue='emo',\n",
    "            kind=\"bar\", aspect=3, height=5, ci='sd')\n",
    "plt.axhline(0., ls='--', c='k')\n",
    "ax = sns.stripplot(x='feature', y='coef', hue='emo', edgecolor='black',\n",
    "                   data=tmp, dodge=True, jitter=False)\n",
    "ax.legend_.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "coef_dfs = dict()\n",
    "results_dm = {'sub': [], 'score': [], 'model': [], 'emo': [], 'feature_space': [], 'type': []}\n",
    "for feature_space in ['nmfdiff', 'pca', 'pcadiff', 'nmf']:\n",
    "    for n_comp in [50, 100, 200]:\n",
    "        fs_name = feature_space + f':{n_comp}'\n",
    "        print('\\n%s' % fs_name)\n",
    "        \n",
    "        coef_dfs[fs_name] = []\n",
    "        for i, f in enumerate(files):\n",
    "\n",
    "            sub = op.basename(f).split('_')[0]\n",
    "            dat = get_Xy_from_tsv(sub.split('-')[1], feature_space=feature_space, n_comp=n_comp)\n",
    "            X, y, intensity = dat['X'], dat['y'], dat['intensity']\n",
    "            preds = cross_val_predict(pipe, X, y, cv=cv, n_jobs=1, method='predict_proba')\n",
    "            scores, coef = cross_val_predict_and_score(pipe, X, y, cv=cv, scoring=scorers[METRIC]['func'])\n",
    "            print(f\"{sub}:\\t {np.round(scores, 3)}\")\n",
    "\n",
    "            if hasattr(scores, '__iter__'):\n",
    "                mult = scores.size\n",
    "            else:\n",
    "                scores = [scores]\n",
    "                mult = 1\n",
    "\n",
    "            coef_df = pd.DataFrame(data=coef, columns=X.columns)\n",
    "            coef_df['sub'] = sub\n",
    "            coef_df['emo'] = le.classes_[:mult]\n",
    "            coef_dfs[fs_name].append(coef_df)\n",
    "\n",
    "            results_dm['sub'].extend([sub] * mult)\n",
    "            results_dm['score'].extend(scores)\n",
    "            results_dm['model'].extend(['self'] * mult)\n",
    "            results_dm['emo'].extend(le.classes_[:mult])\n",
    "            results_dm['feature_space'].extend([fs_name] * mult)\n",
    "            results_dm['type'].extend(['dimred'] * mult)\n",
    "\n",
    "        coef_dfs[fs_name] = pd.concat(coef_dfs[fs_name], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_dm_df = pd.DataFrame(results_dm)\n",
    "sns.catplot(x='feature_space', y='score', hue='emo', data=results_dm_df,\n",
    "            kind=\"bar\", aspect=3, height=5, ci='sd')\n",
    "plt.axhline(0.5, ls='--', c='k')\n",
    "ax = sns.stripplot(x='feature_space', y='score', hue='emo', edgecolor='black',\n",
    "                   data=results_dm_df, dodge=True, jitter=False)\n",
    "ax.legend_.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_nmf = np.load('../behav_data/features/model-nmf_scale-10_weights.npy')\n",
    "plt.figure(figsize=(12, 15))\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i+1)\n",
    "    this = weights_nmf[i, :].reshape((80, 60))\n",
    "    plt.imshow(this)\n",
    "    plt.axis('off')\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_per_emo = coef_dfs['nmf:100'].groupby('emo').mean()\n",
    "fig, ax = plt.subplots(figsize=(20, 15), ncols=6)\n",
    "\n",
    "for i, emo in enumerate(features_per_emo.index):\n",
    "    params = features_per_emo.loc[emo, :].values\n",
    "    these_weights = weights_nmf[:params.size, :]\n",
    "    viz = params @ these_weights\n",
    "    ax[i].imshow(viz.reshape((80, 60)))\n",
    "    ax[i].set_title(emo, fontsize=15)\n",
    "    ax[i].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_pca = np.load('../behav_data/features/model-pca_scale-10_weights.npy')\n",
    "plt.figure(figsize=(12, 15))\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i+1)\n",
    "    this = weights_pca[i, :].reshape((80, 60))\n",
    "    plt.imshow(this)\n",
    "    plt.axis('off')\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_per_emo = coef_dfs['pca:100'].groupby('emo').mean()\n",
    "fig, ax = plt.subplots(figsize=(20, 15), ncols=6)\n",
    "\n",
    "for i, emo in enumerate(features_per_emo.index):\n",
    "    params = features_per_emo.loc[emo, :].values\n",
    "    these_weights = weights_pca[:params.size, :]\n",
    "    viz = params @ these_weights\n",
    "    ax[i].imshow(viz.reshape((80, 60)))\n",
    "    ax[i].set_title(emo, fontsize=15)\n",
    "    ax[i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from scipy.special import softmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "class TheoryKernelClassifier(BaseEstimator, ClassifierMixin):  \n",
    "    \"\"\"An example of classifier\"\"\"\n",
    "\n",
    "    def __init__(self, au_dict, binarize=False, normalize=True, prob_norm='softmax'):\n",
    "        \n",
    "        self.normalize = normalize\n",
    "        self.binarize = binarize\n",
    "        self.prob_norm = prob_norm\n",
    "        self.aus = ['AU1', 'AU10Open', 'AU11', 'AU12', 'AU13', 'AU14', 'AU15',\n",
    "                    'AU16Open', 'AU17', 'AU2', 'AU20', 'AU22', 'AU24', 'AU25',\n",
    "                    'AU26', 'AU27i', 'AU4', 'AU43', 'AU5', 'AU6', 'AU7', 'AU9']\n",
    "        \n",
    "        self.au_dict = au_dict        \n",
    "        for emo, cfg in self.au_dict.items():\n",
    "            if isinstance(cfg, dict):\n",
    "                vec = np.zeros((len(cfg), len(self.aus)))\n",
    "                for i, combi in cfg.items():\n",
    "                    for c in combi:\n",
    "                        vec[i-1, self.aus.index(c)] = 1\n",
    "            else:\n",
    "                vec = np.zeros(len(self.aus))\n",
    "                for c in cfg:\n",
    "                    vec[self.aus.index(c)] = 1\n",
    "\n",
    "            setattr(self, emo, vec)\n",
    "            \n",
    "        le = LabelEncoder().fit(['happy', 'sad', 'disgust', 'angry', 'surprise', 'fear'])\n",
    "        self.le = le\n",
    "        self.ohe = OneHotEncoder(categories='auto', sparse=False)\n",
    "        self.ohe.fit(le.transform(le.classes_)[:, np.newaxis])\n",
    "        self.classes_ = self.le.classes_\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    \n",
    "    def predict_proba(self, X, y=None):\n",
    "        probs = self._predict(X, y)\n",
    "        return probs  \n",
    "\n",
    "    def predict(self, X, y=None, string=False):\n",
    "        probs = self._predict(X, y)\n",
    "        ties = np.squeeze(probs == probs.max())\n",
    "        if np.sum(ties) > 0:\n",
    "            pred = np.random.choice(np.arange(ties.size)[ties])\n",
    "        else:\n",
    "            pred = np.argmax(probs)\n",
    "            \n",
    "        if string:\n",
    "            pred = self.le.inverse_transform([pred])\n",
    "\n",
    "        return pred\n",
    "        \n",
    "    def _predict(self, X, y=None):\n",
    "        \n",
    "        if X.ndim != 2:\n",
    "            raise ValueError(\"X is not 2D.\")\n",
    "        \n",
    "        if X.shape[1] != 22:\n",
    "            raise ValueError(\"This classifier only works with exactly 22 AUs.\")\n",
    "        \n",
    "        if self.binarize:\n",
    "            X = (X > 0).astype(int)\n",
    "            \n",
    "        self.distances = np.zeros((X.shape[0], 6))\n",
    "        for key in self.au_dict.keys():\n",
    "            idx = self.le.transform([key])[0]\n",
    "            vec = getattr(self, key)\n",
    "            \n",
    "            if vec.ndim > 1:\n",
    "                dist_all = np.zeros((vec.shape[0], X.shape[0]))\n",
    "                for i in range(vec.shape[0]):\n",
    "                    sqdist = (X - vec[i, :]) ** 2\n",
    "                    if self.normalize:\n",
    "                        sqdist = sqdist / vec[i, :].sum()\n",
    "                    \n",
    "                    dist_all[i, :] = np.sqrt(np.sum(sqdist, axis=1))\n",
    "                dist = np.mean(dist_all,  axis=0)  # or max() ???\n",
    "            else:\n",
    "                sqdist = (X - vec) ** 2\n",
    "                if self.normalize:\n",
    "                    sqdist = sqdist / vec.sum()\n",
    "                \n",
    "                dist = np.sqrt(np.sum(sqdist, axis=1))\n",
    "            \n",
    "            self.distances[:, idx] = dist\n",
    "        \n",
    "        EPS = 1e-10\n",
    "        self.distances[self.distances == 0] = EPS\n",
    "            \n",
    "        if self.prob_norm == 'softmax':\n",
    "            sm_dist = softmax(1 / self.distances, axis=1)\n",
    "        elif self.prob_norm == 'sum':\n",
    "            sm_dist = (1 / self.distances)\n",
    "            \n",
    "            sm_dist = sm_dist / sm_dist.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "        return sm_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theory_kernels = dict(\n",
    "    \n",
    "    IMotions=dict(\n",
    "        happy=['AU6', 'AU12'],\n",
    "        sad=['AU1', 'AU4', 'AU15'],\n",
    "        surprise=['AU1', 'AU2', 'AU5', 'AU26'],\n",
    "        fear=['AU1', 'AU2', 'AU4', 'AU5', 'AU7', 'AU20', 'AU26'],\n",
    "        angry=['AU4', 'AU5', 'AU7'],  # misses AU23\n",
    "        disgust=['AU9', 'AU15', 'AU16Open']\n",
    "    ),\n",
    "    FaceReader=dict(\n",
    "        happy=['AU6', 'AU12'],\n",
    "        sad=['AU1', 'AU4'],\n",
    "        surprise=['AU1', 'AU2'],\n",
    "        fear=['AU1', 'AU2', 'AU4'],\n",
    "        angry=['AU4', 'AU5'],\n",
    "        disgust=['AU10Open', 'AU25']\n",
    "    ),\n",
    "    Darwin=dict(\n",
    "        happy=['AU6', 'AU12'],\n",
    "        sad=['AU1', 'AU15'],\n",
    "        surprise={\n",
    "            1: ['AU1', 'AU2', 'AU5', 'AU25'],\n",
    "            2: ['AU1', 'AU2', 'AU5', 'AU26']\n",
    "        },\n",
    "        fear=['AU1', 'AU2', 'AU5', 'AU20'],\n",
    "        angry=['AU4', 'AU5', 'AU24'],  # misses AU38\n",
    "        disgust={\n",
    "            1: ['AU10Open', 'AU16Open', 'AU22', 'AU25'],\n",
    "            2: ['AU10Open', 'AU16Open', 'AU22', 'AU26']\n",
    "        }\n",
    "    ),\n",
    "    Matsumoto2008=dict(\n",
    "        happy=['AU6', 'AU12'],\n",
    "        sad={\n",
    "            1: ['AU1', 'AU15'],\n",
    "            2: ['AU4'],\n",
    "            3: ['AU4', 'AU1', 'AU15'],\n",
    "            4: ['AU17'],\n",
    "            5: ['AU17', 'AU1', 'AU15'],\n",
    "            6: ['AU17', 'AU1', 'AU15', 'AU4']\n",
    "        },\n",
    "        surprise={\n",
    "            1: ['AU1', 'AU2', 'AU5', 'AU25'],\n",
    "            2: ['AU1', 'AU2', 'AU5', 'AU26']\n",
    "        },\n",
    "        fear={\n",
    "            1: ['AU1', 'AU2', 'AU4', 'AU5', 'AU20'],\n",
    "            2: ['AU25'],\n",
    "            3: ['AU26']\n",
    "        },\n",
    "        angry={\n",
    "            1: ['AU4', 'AU5', 'AU22', 'AU24'],  # misses AU23\n",
    "            2: ['AU4', 'AU7', 'AU22', 'AU24']   # misses AU23\n",
    "        },\n",
    "        disgust={\n",
    "            1: ['AU9'],\n",
    "            2: ['AU10Open'],\n",
    "            3: ['AU25'],\n",
    "            4: ['AU26'],\n",
    "            5: ['AU9', 'AU25'],\n",
    "            6: ['AU10Open', 'AU26']\n",
    "        }\n",
    "    ),\n",
    "    Keltner2019=dict(\n",
    "        happy=['AU6', 'AU7', 'AU12', 'AU25', 'AU26'],\n",
    "        sad=['AU1', 'AU4', 'AU6', 'AU15', 'AU17'],\n",
    "        surprise=['AU1', 'AU2', 'AU5', 'AU25', 'AU26'],\n",
    "        fear=['AU1', 'AU2', 'AU4', 'AU5', 'AU7', 'AU20', 'AU25'],\n",
    "        angry=['AU4', 'AU5', 'AU17', 'AU24', ],  # misses AU23 \n",
    "        disgust=['AU7', 'AU9', 'AU25', 'AU26']  # misses AU19\n",
    "    ),\n",
    "    Cordaro2008ref=dict(\n",
    "        happy=['AU6', 'AU12'],\n",
    "        sad=['AU1', 'AU4', 'AU5'],\n",
    "        surprise=['AU1', 'AU2', 'AU5', 'AU26'],\n",
    "        fear=['AU1', 'AU2', 'AU4', 'AU5', 'AU7', 'AU20', 'AU26'],\n",
    "        angry=['AU4', 'AU5', 'AU7'],  # misses AU23\n",
    "        disgust=['AU9', 'AU15', 'AU16Open']\n",
    "    ),\n",
    "    Cordaro2008IPC=dict(\n",
    "        happy={\n",
    "            1: ['AU6', 'AU7', 'AU12', 'AU16Open', 'AU25', 'AU26'],\n",
    "            2: ['AU6', 'AU7', 'AU12', 'AU16Open', 'AU25', 'AU27i'],\n",
    "        },\n",
    "        sad=['AU4', 'AU43'],  # misses AU54 (head down)\n",
    "        surprise=['AU1', 'AU2', 'AU5', 'AU25'],\n",
    "        fear=['AU1', 'AU2', 'AU5', 'AU7', 'AU25'],  # also \"jaw\"/\"move back\"\n",
    "        angry=['AU4', 'AU7'],\n",
    "        disgust=['AU4', 'AU6', 'AU7', 'AU9', 'AU10Open', 'AU25', 'AU26']  # also \"jaw\"\n",
    "    )\n",
    ")\n",
    "\n",
    "clf = TheoryKernelClassifier(au_dict=theory_kernels['Darwin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_tk = {'sub': [], 'score': [], 'model': [], 'emo': [], 'feature_space': [], 'type': []}\n",
    "\n",
    "for predef_set in ['IMotions', 'FaceReader', 'Darwin', 'Matsumoto2008', 'Keltner2019', 'Cordaro2008ref', 'Cordaro2008IPC']:\n",
    "    \n",
    "    clf = TheoryKernelClassifier(\n",
    "        theory_kernels[predef_set],\n",
    "        normalize=True,\n",
    "        binarize=False,\n",
    "        prob_norm='softmax'\n",
    "    )\n",
    "    \n",
    "    print(f'\\n{predef_set}')\n",
    "    for i, f in enumerate(files):\n",
    "\n",
    "        sub = op.basename(f).split('_')[0]\n",
    "        dat = get_Xy_from_tsv(sub.split('-')[1], feature_space='AU',\n",
    "                              theory_kernel=predef_set)\n",
    "        X, y, intensity = dat['X'], dat['y'], dat['intensity']\n",
    "            preds = cross_val_predict(clf, X, y, cv=cv, n_jobs=1, method='predict_proba')\n",
    "        scores = scorers[METRIC]['func'](y, preds, per_class=PER_CLASS)\n",
    "        print(f\"{sub}:\\t {np.round(scores, 3)}\")\n",
    "\n",
    "        if hasattr(scores, '__iter__'):\n",
    "            mult = scores.size\n",
    "        else:\n",
    "            scores = [scores]\n",
    "            mult = 1\n",
    "\n",
    "        results_tk['sub'].extend([sub] * mult)\n",
    "        results_tk['score'].extend(scores)\n",
    "        results_tk['model'].extend(['self'] * mult)\n",
    "        results_tk['emo'].extend(le.classes_[:mult])\n",
    "        results_tk['feature_space'].extend(['TK:%s' % predef_set] * mult)\n",
    "        results_tk['type'].extend(['theory'] * mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tk_df = pd.DataFrame(results_tk)\n",
    "g = sns.catplot(\n",
    "    x='feature_space', y='score', hue='emo', data=results_tk_df,\n",
    "    kind=\"bar\", aspect=3, height=5, ci='sd'\n",
    ")\n",
    "plt.axhline(0.5, ls='--', c='k')\n",
    "ax = sns.stripplot(\n",
    "    x='feature_space', y='score', hue='emo', edgecolor='black',\n",
    "    data=results_tk_df, dodge=True, jitter=False\n",
    ")\n",
    "ax.legend_.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tk_df = pd.DataFrame(results_tk)\n",
    "sns.catplot(x='emo', y='score', hue='feature_space', data=results_tk_df,\n",
    "            kind=\"bar\", aspect=3, height=5, ci='sd')\n",
    "plt.axhline(0.5, ls='--', c='k')\n",
    "ax = sns.stripplot(x='emo', y='score', hue='feature_space', edgecolor='black',\n",
    "              data=results_tk_df, dodge=True, jitter=False)\n",
    "ax.legend_.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df = pd.concat((results_df, results_dm_df, results_tk_df), axis=0)\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.catplot(x='feature_space', y='score', data=concat_df, hue='emo',\n",
    "            kind=\"bar\", aspect=3, height=5, ci='sd')\n",
    "\n",
    "plt.axhline(0.5, ls='--', c='k')\n",
    "ax = sns.stripplot(x='feature_space', y='score', hue='emo', edgecolor='black',\n",
    "                   data=concat_df, dodge=True, jitter=False)\n",
    "ax.legend_.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict one subject by others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "METRIC = 'auc_score'\n",
    "CLF = 'lr'\n",
    "N_JOBS = 5\n",
    "GS = False\n",
    "SD = True\n",
    "N_SPLITS = 10\n",
    "PER_CLASS = True\n",
    "\n",
    "if GS:\n",
    "    clf = GridSearchCV(\n",
    "        estimator=clfs[CLF],\n",
    "        param_grid=gridsearch[CLF],\n",
    "        cv=5,\n",
    "        iid=True,\n",
    "        n_jobs=N_JOBS,\n",
    "        scoring=scorers[METRIC]['scorer']\n",
    "    )\n",
    "else:\n",
    "    clf = clfs[CLF]\n",
    "\n",
    "if SD:\n",
    "    pipe = make_pipeline(StandardScaler(), clf)\n",
    "else:\n",
    "    pipe = clfs[CLF]\n",
    "\n",
    "cv = StratifiedKFold(n_splits=N_SPLITS)\n",
    "\n",
    "files = sorted(glob('../behav_data/preproc/*task-expressive*.tsv'))\n",
    "\n",
    "results = {'sub': [], 'score': [], 'model': [], 'emo': [], 'feature_space': [], 'type': []}\n",
    "coef_dfs = dict()\n",
    "\n",
    "for feature_space in ['SA', 'circumplex', 'AU', 'AUxAU', 'AU+SA', 'AUxAU+SA', 'nuisance', 'face_attributes']:\n",
    "    coef_dfs[feature_space] = []\n",
    "    print('\\n%s' % feature_space)\n",
    "\n",
    "    for i, f in enumerate(files):\n",
    "\n",
    "        sub = op.basename(f).split('_')[0]\n",
    "        dat = get_Xy_from_tsv(sub.split('-')[1], feature_space=feature_space)\n",
    "        X, y, intensity = dat['X'], dat['y'], dat['intensity']\n",
    "        scores, coef = cross_val_predict_and_score(pipe, X, y, cv=cv, scoring=scorers[METRIC]['func'])\n",
    "        print(f\"\\n{sub} own model:\\t {np.round(scores, 3)}\")\n",
    "\n",
    "        if hasattr(scores, '__iter__'):\n",
    "            mult = scores.size\n",
    "        else:\n",
    "            scores = [scores]\n",
    "            mult = 1\n",
    "\n",
    "        coef_df = pd.DataFrame(data=coef, columns=X.columns)\n",
    "        coef_df['sub'] = sub\n",
    "        coef_df['emo'] = le.classes_[:mult]\n",
    "        coef_dfs[feature_space].append(coef_df)\n",
    "        \n",
    "        results['sub'].extend([sub] * mult)\n",
    "        results['score'].extend(scores)\n",
    "        results['model'].extend(['self'] * mult)\n",
    "        results['emo'].extend(le.classes_[:mult])\n",
    "        results['feature_space'].extend([feature_space] * mult)\n",
    "        results['type'].extend(['AU-based'] * mult)\n",
    "        \n",
    "        all_others = [get_Xy_from_tsv(op.basename(fo).split('_')[0].split('-')[1], feature_space=feature_space)\n",
    "                      for fo in files if fo != f]\n",
    "        Xall = pd.concat([a['X'] for a in all_others], axis=0)\n",
    "        yall = pd.concat([a['y'] for a in all_others], axis=0)\n",
    "        \n",
    "        scores, coefs = cross_val_predict_and_score(\n",
    "            pipe, Xall, yall, cv=cv, scoring=scorers[METRIC]['func'],\n",
    "            X_cv=X, y_cv=y\n",
    "        )\n",
    "        print(f\"Predicted by all others: {np.round(scores, 3)}\")\n",
    "\n",
    "        coef_df = pd.DataFrame(data=coef, columns=X.columns)\n",
    "        coef_df['sub'] = sub\n",
    "        coef_df['emo'] = le.classes_[:mult]\n",
    "        coef_dfs[feature_space].append(coef_df)\n",
    "        \n",
    "        results['sub'].extend([sub] * mult)\n",
    "        results['score'].extend(scores)\n",
    "        results['model'].extend(['all_other'] * mult)\n",
    "        results['emo'].extend(le.classes_[:mult])\n",
    "        results['feature_space'].extend([feature_space] * mult)\n",
    "        results['type'].extend(['AU-based'] * mult)\n",
    "        \n",
    "        for ii, fo in enumerate(files):\n",
    "            if f == fo:\n",
    "                continue\n",
    "\n",
    "            sub_other = op.basename(fo).split('_')[0]\n",
    "            dat = get_Xy_from_tsv(sub_other.split('-')[1], feature_space=feature_space)\n",
    "            Xo, yo = dat['X'], dat['y']\n",
    "            scores, coefs = cross_val_predict_and_score(\n",
    "                pipe, Xo, yo, cv=cv, scoring=scorers[METRIC]['func'],\n",
    "                X_cv=X, y_cv=y\n",
    "            )\n",
    "            #print(f\"Predicted by {sub_other}:\\t {np.round(scores, 3)}\")\n",
    "\n",
    "            coef_df = pd.DataFrame(data=coef, columns=X.columns)\n",
    "            coef_df['sub'] = sub\n",
    "            coef_df['emo'] = le.classes_[:mult]\n",
    "            coef_dfs[feature_space].append(coef_df)\n",
    "\n",
    "            results['sub'].extend([sub] * mult)\n",
    "            results['score'].extend(scores)\n",
    "            results['model'].extend(['other'] * mult)\n",
    "            results['emo'].extend(le.classes_[:mult])\n",
    "            results['feature_space'].extend([feature_space] * mult)\n",
    "            results['type'].extend(['AU-based'] * mult)\n",
    "\n",
    "    coef_dfs[feature_space] = pd.concat(coef_dfs[feature_space], axis=0)\n",
    "    \n",
    "df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "sns.catplot(x='emo', y='score', hue='model', data=results_df,\n",
    "            kind=\"bar\", aspect=3, height=5, ci='sd', row='feature_space')\n",
    "plt.axhline(0.5, ls='--', c='k')\n",
    "#ax = sns.stripplot(x='feature_space', y='score', hue='emo', edgecolor='black',\n",
    "#                   data=results_df, dodge=True, jitter=False)\n",
    "#ax.legend_.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=4, ncols=3, sharex=True, sharey=True, figsize=(20, 20))\n",
    "subs = df['sub'].unique()\n",
    "for i, ax in enumerate(axes.flatten()[:-1]):\n",
    "    this_sub = subs[i]\n",
    "    tmp = df.query(\"sub == @this_sub\")\n",
    "    tmp.loc[tmp['model'].str.contains('sub'), 'model'] = 'other'\n",
    "    #g = sns.stripplot(x='emo', y='score', hue='model', data=tmp, ax=ax)\n",
    "    \n",
    "    sns.stripplot(x='emo', y='score', hue='model', data=tmp.query(\"model == 'other'\"),\n",
    "                  zorder=1, ax=ax, alpha=0.3, color='black')\n",
    "    \n",
    "    sns.pointplot(x='emo', y='score', data=tmp.query(\"model == 'other'\"),\n",
    "                  markers=\"x\", join=False, ci=None, palette=\"dark\", ax=ax, dodge=0.5)\n",
    "    \n",
    "    sns.pointplot(x='emo', y='score', data=tmp.query(\"model == 'self'\"),\n",
    "                  markers=\"*\", join=False, ci=None, palette=\"dark\", ax=ax, scale=1.5)\n",
    "    \n",
    "    sns.pointplot(x='emo', y='score', data=tmp.query(\"model == 'all_other'\"),\n",
    "                  markers=\"o\", join=False, ci=None, palette=\"dark\", ax=ax)\n",
    "    \n",
    "    ax.get_legend().remove()\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_title(this_sub, fontsize=15)\n",
    "    #ax.set_ylim(-0.3, 0.6)\n",
    "\n",
    "sns.despine()\n",
    "handles = [\n",
    "    Line2D([0], [0], marker='x', color='black', label='mean(other)', linestyle='None', markersize=13),\n",
    "    Line2D([0], [0], marker='*', color='black', label='self', linestyle='None', markersize=13),\n",
    "    Line2D([0], [0], marker='o', color='black', label='all others', linestyle='None', markersize=13),          \n",
    "]\n",
    "axes[0, 1].legend(handles=handles, frameon=False, loc='center', fontsize=15)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "sns.pointplot(x='sub', y='score', hue='model', data=df,\n",
    "              markers=[\"x\", \"o\", \"*\"], scale=1.2, join=False, ci=None, palette=\"dark\")\n",
    "\n",
    "sns.pointplot(x='sub', y='ceiling', data=ceiling, ci=False, join=False, markers='_', scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "sns.pointplot(x='emo', y='score', hue='model', data=df,\n",
    "              markers=[\"x\", \"o\", \"*\"], scale=1.2, join=False, ci=None, palette=\"dark\")\n",
    "sns.pointplot(x='emo', y='ceiling', data=ceiling, ci=False, join=False, markers='_', scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions of social attribute ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted(glob('../behav_data/preproc/*task-neutral*.tsv'))\n",
    "\n",
    "fig, axes = plt.subplots(nrows=len(files), sharex=True, sharey=False, ncols=5, figsize=(15, 3*len(files)))\n",
    "for i, f in enumerate(files):\n",
    "    sub = op.basename(f).split('_')[0]\n",
    "    sdata = pd.read_csv(f, sep='\\t')\n",
    "    \n",
    "    for ii, attr in enumerate(['valence', 'arousal', 'attractiveness', 'dominance', 'trustworthiness']):\n",
    "        sns.distplot(np.nan_to_num(sdata[f'rating_{attr}']), ax=axes[i, ii])\n",
    "        axes[i, ii].set_xlim(-1, 1)\n",
    "        \n",
    "sns.despine()\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
